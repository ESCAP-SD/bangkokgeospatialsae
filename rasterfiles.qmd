---
title: "Geospatial/SAE workshop"
subtitle: "Raster files"
author: "Josh Merfeld"
date: "October 2024"
date-format: "MMMM YYYY"

format: 
  revealjs:
    self-contained: true
    slide-number: false
    progress: false
    theme: [serif, custom.scss]
    width: 1500
    height: 1500*(9/16)
    code-copy: true
    code-fold: show
    code-overflow: wrap
    highlight-style: github
execute:
  echo: true
  warnings: false
---


```{r}
#| label: setup
#| include: false
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

library(tidyverse)
library(sf)
library(terra)
library(tidyterra)
library(cowplot)
library(kableExtra)
library(haven)

Sys.setenv("RETICULATE_PYTHON" = "~/Library/r-miniconda-arm64/envs/RGEEDIM/bin/python3.9")
library(rgeedim)
# short duration token
gd_authenticate(auth_mode = "gcloud")
# initialize and should be good to go 
gd_initialize()

kdisgreen <- "#006334"
accent <- "#340063"
accent2 <- "#633400"
kdisgray <- "#A7A9AC"

```



## Rasters

- We've discussed shapefiles
  -  Now let's talk about rasters!<br><br>
  
- Rasters are a different type of geospatial data
  - They are made up of a grid of cells
  - Each cell has a value






## Example raster grid - how much info do we need?

```{r}
#| echo: false
#| include: true
#| fig-align: center
# create example grid
main <- ggplot() +
  geom_hline(yintercept = 1:10, color = "black") +
  geom_vline(xintercept = 1:10, color = "black") +
  theme_bw() +
  labs(x = "X", y = "Y") +
  scale_x_continuous(breaks = 1:10, minor_breaks = NULL) +
  scale_y_continuous(breaks = 1:10, minor_breaks = NULL)

gridexample <- ggdraw() +
  draw_plot(
    {
      main +
        coord_sf(
          xlim = c(0.99, 10.01),
          ylim = c(0.99, 10.01),
          expand = FALSE)
    }
)
gridexample
```


- Here's a grid.
  - How many points do we need?






## Example raster grid - how much info do we need?

```{r}
#| echo: false
#| include: true
#| fig-align: center


gridexample
```


- Need to know location of one grid cell...
  - And the size of each grid!






## How much info do we need?

- In other words, we do not need a point for every raster cell<br><br>

- We just need to know:
  - The location of one cell
  - The size of each cell
    - This is called the `resolution` of the raster<br><br>
    
- Example:
  - I know the first grid cell in bottom left is at (0, 0)
  - I know each grid cell is 1 meter by 1 meter (the resolution)
  - Then I know the exact location of every single grid cell






## Population in Cotonou, Benin

```{r}
#| echo: false
#| include: true
#| fig-align: center

library(tidyterra)

tif <- rast("rastersdata/beninpop.tif")

ggplot() +
  geom_spatraster(data = tif, ) + 
  scale_fill_distiller("Population\ncount", palette = "Spectral", na.value = "white") +
  theme_bw() +
  labs(subtitle = "Population in Cotonou, Benin")

```

- What are the white values?






## Population in Cotonou, Benin

- Here's the information for this raster
  - What's the resolution? What are the units?


```{r}
#| echo: false
#| include: true
#| fig-align: center

tif

```






## Rasters

- Rasters are defined by the grid layout and the resolution
  - Grid cells are sometimes called pixels (just like images, which are often rasters!)<br><br>

- There are many different file types for rasters
  - `.tif` or `.tiff` (one of the most common)
  - `.nc` (NetCDF, common for very large raster data)
  - Image files, e.g. `png`, `jpg`, etc.<br><br>






## Reading rasters in R

- Reading rasters is also quite easy!
  - Going to use the `terra` package for it
    - Note: can use `terra` for shapefiles, too
  - `rastersdata/beninpop.tif` is a raster of population counts in Benin

```{r}
#| echo: true
#| include: true
#| fig-align: center
library(terra)

# this is the raster for Cotonou, Benin
cotonou <- rast("rastersdata/beninpop.tif")
cotonou
```







## Plotting rasters

::: columns
::: {.column width="45%"}

- Plotting rasters only with `terra` is a bit of a pain
  - Can't use `ggplot`
  - So, I load another package that lets me use `ggplot` with rasters
    - `tidyterra`

```{r}
#| echo: true
#| eval: false
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou)
```

:::

::: {.column width="55%"}

```{r}
#| echo: false
#| include: true
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou)
```

:::
:::







## Making it nicer

::: columns
::: {.column width="45%"}

```{r}
#| echo: true
#| eval: false
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou) + 
  # distiller is for continuous values
  # but we can use palettes!
  # I like spectral a lot
  scale_fill_distiller("Population\ncount", 
    palette = "Spectral", na.value = "white") +
  theme_bw() +
  labs(subtitle = "Population in Cotonou, Benin")
```

:::

::: {.column width="55%"}

```{r}
#| echo: false
#| include: true
#| fig-align: center
library(tidyterra)

ggplot() +
  geom_spatraster(data = cotonou) + 
  scale_fill_distiller("Population\ncount", palette = "Spectral", na.value = "white") +
  theme_bw() +
  labs(subtitle = "Population in Cotonou, Benin")

```

:::
:::







## Extracting raster data to shapefiles

- Let's go back to our use case:
  - We want to estimate a sub-area model at the EA level in Malawi
  - This means we need to extract raster data to the EA level
  - We can do this with `terra`, `sf`, and `exactextractr`
    - `terra` has its own method, but i find `exactextractr` to be MUCH faster<br><br>
    
- Let's start by looking at the raster I've uploaded to the `rastersdata`: `mwpop.tif`







## Give it a try

::: columns
::: {.column width="45%"}

- Try to load it into R using terra, then plot it with tidyterra and ggplot

```{r}
#| echo: true
#| eval: false
#| fig-align: center
#| code-fold: show

tif <- rast("rastersdata/mwpop.tif")

ggplot() +
  geom_spatraster(data = tif) + 
  scale_fill_distiller("Population\ncount", 
    palette = "Spectral", na.value = "white") +
  theme_bw() +
  labs(subtitle = "Population in Northern Malawi")
```

:::

::: {.column width="55%"}

![](rastersassets/mwpop.png){fig-align="center"}

:::
:::







## Give it a try

::: columns
::: {.column width="45%"}

- I actually don't like that map! It's too hard to see because of all the low values.
- So let's take logs, instead!
  - Note that all the zeros become missing (can't log zero)

```{r}
#| echo: true
#| eval: false
#| fig-align: center
#| code-fold: show

tif <- rast("rastersdata/mwpop.tif")

ggplot() +
  geom_spatraster(data = log(tif)) + 
  scale_fill_distiller("Population\ncount (log)", 
    palette = "Spectral", na.value = "white") +
  theme_bw() +
  labs(subtitle = "Population in Northern Malawi")
```

:::

::: {.column width="55%"}

![](rastersassets/mwpop2.png){fig-align="center"}

:::
:::







## We want to extract the .tif values to the .shp

```{r}
#| echo: false
#| eval: true
#| fig-align: center

tif <- rast("rastersdata/mwpop.tif")
adm4 <- read_sf("rastersdata/mw4.shp")

g1 <- ggplot() +
  geom_spatraster(data = tif) + 
  scale_fill_distiller("Population\ncount (log)", 
    palette = "Spectral", na.value = "white") +
  theme_bw() +
  labs(subtitle = "Population in Northern Malawi")
g2 <- ggplot() +
  geom_sf(data = adm4, color = "black", fill = "transparent", lwd = 0.1) + 
  theme_bw() +
  labs(subtitle = "EAs (admin4) Northern Malawi")

plot_grid(g1, NA, g2, ncol = 3, rel_widths = c(1, 0.05, 1))
```







## Let's do it with `exactextractr`

```{r}
#| echo: true
#| eval: false
#| include: true
#| fig-align: center
library(exactextractr)

tif <- rast("rastersdata/mwpop.tif")
adm4 <- read_sf("rastersdata/mw4.shp")
# make sure they are in the same CRS! (they already are, but just in case)
# st_transform is for the sf object
adm4 <- st_transform(adm4, crs = crs(tif))

# extract the raster values to the shapefile
# we are going to SUM, and add the EA_CODE from the shapefile to the result
extracted <- exact_extract(tif, adm4, fun = "sum", append_cols = "EA_CODE")
```

```{r}
#| echo: false
#| eval: true
#| include: false
#| fig-align: center
library(exactextractr)

tif <- rast("rastersdata/mwpop.tif")
adm4 <- read_sf("rastersdata/mw4.shp")
# make sure they are in the same CRS! (they already are, but just in case)
# st_transform is for the sf object
adm4 <- st_transform(adm4, crs = crs(tif))

# extract the raster values to the shapefile
# we are going to SUM, and add the EA_CODE from the shapefile to the result
extracted <- exact_extract(tif, adm4, fun = "sum", append_cols = "EA_CODE")
# save it!
write_csv(extracted |> rename(pop = sum), "rastersdata/mwpopEAs.csv")
```

```{r}
#| echo: true
#| eval: true
#| include: true
#| fig-align: center

head(extracted)
```







## Now we can join the extracted data to the shapefile


::: columns
::: {.column width="45%"}


```{r}
#| echo: true
#| eval: false
#| include: true
#| fig-align: center

# join
adm4 <- adm4 |>
  left_join(extracted, by = "EA_CODE")

# plot it!
ggplot() +
  geom_sf(data = adm4, aes(fill = sum), 
    color = "black", lwd = 0.01) +
  scale_fill_distiller("Population\ncount", 
    palette = "Spectral", na.value = "white") +
  theme_bw() +
  labs(subtitle = "Population in EAs")

```

:::

::: {.column width="55%"}

![](rastersassets/mwpopEAs.png){fig-align="center"}

:::
:::







## Now it's your turn

- Here's your task:
  - Search for "worldpop population counts"
    - Should be the first result (link: [https://hub.worldpop.org/project/categories?id=3](https://hub.worldpop.org/project/categories?id=3))
  - Scroll down the page, click on "unconstrained individual countries 2000-2020 UN adjusted (1km resolution)

![](rastersassets/worldpoppage1.png){fig-align="center"}






## Now it's your turn

- Here's your task:
  - Search for "worldpop population counts"
    - Should be the first result (link: [https://hub.worldpop.org/project/categories?id=3](https://hub.worldpop.org/project/categories?id=3))
  - Scroll down the page, click on "unconstrained individual countries 2000-2020 UN adjusted (1km resolution)
  - Then, search for a country (maybe yours?)

![](rastersassets/worldpoppage2.png){fig-align="center"}





## Now it's your turn

- Here's your task:
  - Search for "worldpop population counts"
    - Should be the first result (link: [https://hub.worldpop.org/project/categories?id=3](https://hub.worldpop.org/project/categories?id=3))
  - Scroll down the page, click on "unconstrained individual countries 2000-2020 UN adjusted (1km resolution)
  - Then, search for a country (maybe yours?)
  - Click on "Data & Resources" for 2020
  - Scroll down to the bottom of the page and download the .tif





## Now it's your turn

- Load the .tif into R using `terra`
- Plot the raster using `tidyterra` and `ggplot`
  - Make it look nice!





## Let's keep going!

- Now you need to find a shapefile for the same country
- This will be a bit less straightforward
  - Search for "shapefile COUNTRY humdata"
  - You should find a link to the Humanitarian Data Exchange
  - Click on it and see if it has shapefiles for your country of choice
  - If so, download a shapefile (it can be at a higher admin level)
    - If not, raise your hand and I'll come help you find a shapefile
  - Load it into R and plot it!





## One last thing

- You have the population tif and the shapefile
- Extract the population data (using sum, don't forget!) to the shapefile
  - Use `append_cols` and make sure you choose the correct identifier!
- Join the data to the shapefile
- Plot the shapefile with the population data
  - Make it look nice!





## What can you do with that data?

- Now you have a shapefile with population data
- You can save it as a `.csv` and use it in your analysis!
  - We'll get to this point eventually.
  - We will also discuss adding the survey data and then estimating a sub-area model





# Finding rasters {background-image="rastersassets/ndvibackground.png" background-position="right"}


## Where can you find rasters?

- Depending on the variable, rasters are sometimes quite easy to find!
  - We already saw one example: WorldPop (population counts)<br><br>

- There are two large online repositories:
  - [Google Earth Engine](https://developers.google.com/earth-engine/datasets)
  - [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/)
    - This one is newer and has less data (for now)



## {background-image="rastersassets/gee1.png"}

## Google Earth Engine

- Google Earth Engine has _a lot_ of data.

- Let's see some examples
    
    
  

## Surface temperature {#color-slide2 background-image="rastersassets/geeexample1.gif"}

```{css, echo=FALSE}
#color-slide2, 
#color-slide2 h2 {
 color: white;
 text-align: left;
}
```


## Climate {#color-slide3 background-image="rastersassets/geeexample2.gif"}

```{css, echo=FALSE}
#color-slide3, 
#color-slide3 h2 {
 color: white;
 text-align: left;
}
```

## Land cover {#color-slide4 background-image="rastersassets/geeexample3.gif"}

```{css, echo=FALSE}
#color-slide4, 
#color-slide4 h2 {
 color: white;
 text-align: left;
}
```

## Imagery {#color-slide5 background-image="rastersassets/geeexample4.jpg"}

```{css, echo=FALSE}
#color-slide5, 
#color-slide5 h2 {
 text-align: left;
}
```



## GeoLink

- I'm going to start with some example code for a package called `GeoLink`
  - This package actually uses Microsoft Planetary Computer
  - It is very very easy to use
  - The problem: as of now, it only has a few datasets

```{r}
#| eval: false
#| echo: true

# we have to download it differently
devtools::install_github("SSA-Statistical-Team-Projects/GeoLink")

```



## The actual code is much easier to use than GEE!

```{r}
#| eval: false
#| echo: true

library(GeoLink)
library(sf)

adm4 <- read_sf("mw4.shp")
# CHIRPS is rainfall data
adm4_chirps <- geolink_chirps(time_unit = "month",
  start_date = "2019-01-01",
  end_date = "2019-03-01",
  shp_dt = adm4,
  extract_fun = "mean")
summary(adm4_chirps)

```

- Keep an eye on this package! It will be very useful when it has more datasets




## Google Earth Engine

- First things first: you need an account!<br><br>

- Go to [https://earthengine.google.com/](https://earthengine.google.com/) and sign up
  - Top-right corner: `Get Started`
  - Next page: `Create account`<br><br>
  
- I'll give you all a couple minutes to get this set up. Let me know if you have problems



## Let's look at a dataset

- On the [https://earthengine.google.com/](https://earthengine.google.com/) page:
  - Click `View all datasets` (near the top)
  - Search for `lights`
  - We want `VIIRS Nighttime Day/Night Annual Band Composites V2.1`

![](rastersassets/gee2.png){fig-align="center"}



## Basic information about the dataset

![](rastersassets/gee3.png){fig-align="center"}



## Raster bands - They can have more than one!

![](rastersassets/gee4.png){fig-align="center"}





## We need to download python... sorry!

- The first time you use `rgeedim`, you will need to install python
  - The easiest way is to search `download miniconda`
  - One of the first links should be [https://docs.anaconda.com/miniconda/](https://docs.anaconda.com/miniconda/)
  - Go down to "Latest Miniconda installer links" and select the correct link for your platform (e.g. Windows)
  - Once it finishes downloading, please go do your downloads folder and double click to install<br><br>
  
- Let's take five minutes to download and install python



## Downloading the data is... a pain

- Actually getting the data is a bit of a pain
  - Unless you know Javascript!<br><br>
  
- A lot of people use libraries in `R` (or python) to download data, instead.
  - All of them are a bit cumbersome.
    - Especially true in `R`, because we need to use python!
  - We are going to use `rgeedim`
  - Go ahead and install it using `install.packages("rgeedim")`
  - Load it using `library(rgeedim)`
    - Type `Yes` when asked to create a default Python environment




## The code

```{r}
#| eval: false
#| echo: true

library(rgeedim)
gd_install()
gd_authenticate(auth_mode = "notebook")
```

- After `gd_authenticate()`, your browser should open.
  - You'll need to sign in to your Google account
  - Continue through the prompts and make sure you select all access




## The code

```{r}
#| eval: false
#| echo: true

library(rgeedim)
gd_install()
gd_authenticate(auth_mode = "notebook")
```

- After `gd_authenticate()`, your browser should open.
  - You'll need to sign in to your Google account
  - Continue through the prompts and make sure you select all access



## The code

::: columns
::: {.column width="45%"}

- You'll arrive at this page.
- Click the `Copy` button

```{r}
#| eval: false
#| echo: true

library(rgeedim)
gd_install()
gd_authenticate(auth_mode = "notebook")
```

- Then go back to RStudio, and paste (ctrl + v) the code into the console

:::

::: {.column width="55%"}

![](rastersassets/geelogin.png){fig-align="center"}

:::
:::




## The code

```{r}
#| eval: false
#| echo: true

library(rgeedim)
#gd_install() # You SHOULD NOT need to do this on each new session
gd_authenticate(auth_mode = "notebook") # need to do this
gd_initialize() # and you need to do this
```

- After you do `gd_install()` once, you should be good
  - You will need to do `gd_authenticate()` and `gd_initialize()` each time you start a new session




## Downloading the data - still not straightforward!

- First, we need to create a "bounding box"
  - This is the area of the globe we want to search
  - We will use the Malawi shapefile for this
  - The "bounding box" is a rectangle that completely contains the shapefile
  
```{r}
#| eval: true
#| echo: true

# load shapefile
malawi <- read_sf("rastersdata/mw4.shp")
# this creates the bounding box
bbox <- st_bbox(malawi)
bbox

```







## Basic information about the dataset

- Remember I said we'd need this again?

![](rastersassets/gee3b.png){fig-align="center"}








## Image collections vs. images

- One key thing to understand about GEE is the difference between image collections and images<br><br>

- An image collection is what it sounds like: a collection of images
  - The key is that we won't download image collections
  - We'll download individual images
  - So we need to find the images!
  







## Get images from the collection

```{r}
#| eval: true
#| echo: true

x <- gd_collection_from_name("NOAA/VIIRS/DNB/ANNUAL_V21") |>
  gd_search(region = bbox)
gd_properties(x)

```

- The survey data I have from Malawi is 2019/2020, so let's download the 2019-01-01 data
  - We want to use the id: `NOAA/VIIRS/DNB/ANNUAL_V21/20190101`
  







## We can FINALLY download the raster!



```{r}
#| eval: true
#| echo: true

x <- gd_image_from_id("NOAA/VIIRS/DNB/ANNUAL_V21/20190101") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution of raster is only 500, so no reason to go lower
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    silent = FALSE
  )
# we downloaded the raster and called it x
# so let's load it using terra!
x <- rast(x)
# here it is!
x
```




  







## Quick note: we downloaded many bands!



```{r}
#| eval: true
#| echo: true
#| class-output: hscroll

names(x)

# but we really only want average nightlights
# so here's how you can download just the average
x <- gd_image_from_id("NOAA/VIIRS/DNB/ANNUAL_V21/20190101") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution of raster is only 500, so no reason to go lower
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    silent = FALSE,
    bands = list("average"),
  )
# we downloaded the raster and called it x
# so let's load it using terra!
x <- rast(x)
# here it is!
x
```



  







## What does it look like?


::: columns
::: {.column width="45%"}

```{r}
#| eval: false
#| echo: true
adm4 <- read_sf("rastersdata/mw4.shp")
ggplot() +
  geom_spatraster(data = x) +
  scale_fill_distiller("Nightlights", 
    palette = "Spectral") +
  geom_sf(data = adm4, 
    color = "white",
    lwd = 0.01,
    alpha = 0.5,
    fill = "transparent") +
  theme_bw() +
  labs(subtitle = "Nightlights in Malawi")
```

- Note what the "bounding box" does!
  - `st_bbox` as a reminder

:::

::: {.column width="55%"}

![](rastersassets/rgeedim1.png){fig-align="center"}

:::
:::


  
  
  
  



  







## We want to extract NTL to the shapefile


```{r}
#| eval: false
#| echo: true
adm4 <- read_sf("rastersdata/mw4.shp")
# exact_extract
library(exactextractr)
ntlextract <- exact_extract(x, adm4, fun = "mean", append_cols = "EA_CODE")
head(ntlextract)
# save it!
write_csv(ntlextract |> rename(ntl = mean), "rastersdata/mwntlEAs.csv")

```








## Now it's your turn!

- We want to download NDVI data for Malawi
  - We want the `Terra Vegetation Indices 16-Day Global 500m` data (you can just search this)
  - Download the first observation from 2019
  - Extract it to the mw4 shapefile!




```{r}
#| eval: false
#| echo: true
#| code-fold: show

# load shapefile
malawi <- read_sf("rastersdata/mw4.shp")
# this creates the bounding box
bbox <- st_bbox(malawi)

x <- gd_collection_from_name("MODIS/061/MOD13A1") |>
  gd_search(region = bbox)
gd_properties(x)
# the ID for the first image of 2019 is MODIS/061/MOD13A1/2019_01_01

```



## Let's download that id


::: columns
::: {.column width="55%"}

```{r}
#| eval: false
#| echo: true

x <- gd_image_from_id("MODIS/061/MOD13A1/2019_01_01") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    bands = list("NDVI") # only download NDVI
  )
x <- rast(x) # load raster
ndviextract <- exact_extract(x, malawi, fun = "mean", append_cols = "EA_CODE")
malawi <- malawi |>
  left_join(ndviextract |> rename(ndvi = mean), by = "EA_CODE")
ggplot() +
  geom_sf(data = malawi, aes(fill = ndvi), lwd = 0.01,) +
  scale_fill_distiller("NDVI", 
    palette = "Greens", direction = 1) +
  theme_bw()
```

```{r}
#| eval: true
#| echo: false
#| include: false
#| class-output: hscroll

x <- gd_image_from_id("MODIS/061/MOD13A1/2019_01_01") |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    bands = list("NDVI") # only download NDVI
  )
x <- rast(x) # load raster
ndviextract <- exact_extract(x, malawi, fun = "mean", append_cols = "EA_CODE")
malawi <- malawi |>
  left_join(ndviextract |> rename(ndvi = mean), by = "EA_CODE")
ggplot() +
  geom_sf(data = malawi,
    aes(fill = ndvi),
    lwd = 0.01,) +
  scale_fill_distiller("NDVI", 
    palette = "Greens", direction = 1) +
  theme_bw()
```


:::

::: {.column width="45%"}

![](rastersassets/rgeedimndvi.png){fig-align="center" width="65%"}

:::
:::



  







## This returns a LOT of results - search by date!


```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# load shapefile
malawi <- read_sf("rastersdata/mw4.shp")
# this creates the bounding box
bbox <- st_bbox(malawi)

# so let's filter by date!
x <- gd_collection_from_name("MODIS/061/MOD13A1") |>
  gd_search(region = bbox,
    start_date = "2019-01-01",
    end_date = "2019-12-31")
gd_properties(x)

```

  







## Let's look at the dates


```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

gd_properties(x)$date

```

- What should we download?
  - NDVI is a vegetation index, which means it varies quite a bit throughout the year
  - We could take average NDVI throughout the year
  - Or we could take NDVI at a specific time of year
  - Or we could take the max... or the min... or all of the above!
  
- Let's download one raster PER MONTH
  - How?
  
  
  
  
  
  
## Let's look at the dates

- There are `r length(gd_properties(x)$date)` dates
- This code is a bit complicated, so let me explain


```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll
# get the dates
dates <- gd_properties(x)$date
# here are the months
months <- month(dates)
ids <- c() # this creates an empty vector
for (m in 1:12){ # this "for loop" loops through each month (1 through 12)
  ids <- c(ids, which(months == m)[1]) # it then takes the LOCATION of the FIRST VALUE equal to m
}
ids <- gd_properties(x)$id[ids] # this gets the image ids at those locations
ids # Now we have all the ids we want to download!
```




## Here's how I would do this


```{r}
#| eval: false
#| echo: true
#| code-fold: show
#| class-output: hscroll

adminareas <- malawi |>
  as_tibble() |>
  select(EA_CODE)

for (i in 1:length(ids)){
  x <- gd_image_from_id(ids[i]) |>
  gd_download(
    filename = "temp.tif",
    region = bbox, # region is our bbox
    scale = 500, # resolution
    crs = 'EPSG:4326', # lat/lon
    overwrite = TRUE, # overwrite if it exists
    bands = list("NDVI") # only download NDVI
  )
  x <- rast(x) # load raster
  ndviextract <- exact_extract(x, malawi, fun = "mean", append_cols = "EA_CODE")
  colnames(ndviextract) <- c("EA_CODE", paste0("NDVI_", i))
  adminareas <- adminareas |>
    left_join(ndviextract, by = "EA_CODE")
}

```





## But that's difficult, so I've uploaded the data!

- That's a hard thing to wrap your head around if you're new to R
  - If you want to download them, you can just do them one at a
  - I've uploaded the data:
    - `rastersdata/ndviallmonths.csv`
    - Go ahead and load it and take a look at it

```{r}
#| eval: true
#| echo: true
#| code-fold: true
#| class-output: hscroll

ndviall <- read_csv("rastersdata/ndviallmonths.csv")
ndviall

```





## Let's create some new variables

- Let's create three new NDVI variables:
  - Annual minimum
  - Annual maximum
  - Annual average
- New `R` function: apply!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# create new variable called min. the "1" means across ROWS.
ndviall$ndvimin <- apply(ndviall |> select(starts_with("NDVI")), 1, min, na.rm = TRUE)
# max
ndviall$ndvimax <- apply(ndviall |> select(starts_with("NDVI")), 1, max, na.rm = TRUE)
# mean
ndviall$ndvimean <- apply(ndviall |> select(starts_with("NDVI")), 1, mean, na.rm = TRUE)
# just keep those
ndviall <- ndviall |>
  select(EA_CODE, ndvimin, ndvimax, ndvimean)
# save it!
write_csv(ndviall, "rastersdata/ndviclean.csv")

```







## One last step

- We need to get the codes for the survey data!
- I've uploaded the survey data for Malawi
  - `rastersdata/ihs5_consumption_aggregate.dta`
  - Go ahead and load it (remember to use `haven`)

```{r}
#| eval: true
#| echo: true
#| code-fold: true
library(haven)
ihs5 <- read_dta("rastersdata/ihs5_consumption_aggregate.dta")
head(ihs5)
```









## One last step

```{r}
#| eval: true
#| echo: true
#| code-fold: show
head(ihs5)
```

- Note that in this case, we already have the ea codes in the survey data
  - So we can just join the two datasets!









## Collapse to EA_CODE

```{r}
#| eval: true
#| echo: true
#| code-fold: show
library(stats) # this is for weighted.mean
ihs5ea <- ihs5 |>
  rename(EA_CODE = ea_id) |>
  group_by(EA_CODE) |>
  # Note that this is a weighted mean!
  summarize(poor = stats::weighted.mean(x = poor, w = hh_wgt*adulteq, na.rm = TRUE), # weighted mean
    total_weights = sum(hh_wgt*adulteq, na.rm = TRUE), # sum total weights
    total_obs = n()) # total observations (households) in the EA
head(ihs5ea)
```









## But what if you don't have an identifier?

- Sometimes you have GPS coordinates but not matching identifiers
  - Good news! We can use geospatial tools to match the coordinates to the shapefile
  
```{r}
#| eval: true
#| echo: true
#| code-fold: show
ihs5coords <- read_dta("rastersdata/householdgeovariables_ihs5.dta")
# turn it into an sf object
ihs5coords <- ihs5coords |>
  filter(!is.na(ea_lon_mod)) |> # get rid of any missing values (can't use them)
  st_as_sf(coords = c("ea_lon_mod", "ea_lat_mod"), crs = 4326)
head(ihs5coords)
```









## Here's how it looks


::: columns
::: {.column width="65%"}

```{r}
#| eval: false
#| echo: true
#| code-fold: show
ihs5coords <- read_dta("rastersdata/householdgeovariables_ihs5.dta")
# turn it into an sf object
ihs5coords <- ihs5coords |>
  filter(!is.na(ea_lon_mod)) |> # get rid of any missing values (can't use them)
  st_as_sf(coords = c("ea_lon_mod", "ea_lat_mod"), crs = 4326)
admin4 <- read_sf("rastersdata/mw4.shp")
ggplot() + 
  geom_sf(data = admin4, color = "transparent", lwd = 0.01) +
  geom_sf(data = ihs5coords, color = "red") +
  theme_bw()
```

:::

::: {.column width="35%"}

![](rastersassets/ihs5coords.png){fig-align="center" width="65%}

:::
:::










## We can join them using `st_join`

```{r}
#| eval: true
#| echo: true
#| code-fold: show
ihs5coords <- read_dta("rastersdata/householdgeovariables_ihs5.dta")
# turn it into an sf object
ihs5coords <- ihs5coords |>
  filter(!is.na(ea_lon_mod)) |> # get rid of any missing values (can't use them)
  st_as_sf(coords = c("ea_lon_mod", "ea_lat_mod"), crs = 4326)
admin4 <- read_sf("rastersdata/mw4.shp")
ihs5coords <- st_join(ihs5coords, admin4)
# and that's it!
head(ihs5coords)
```









## Now we need to add the EA_CODE to the poverty data

```{r}
#| eval: true
#| echo: true
#| code-fold: show
ihs5 <- read_dta("rastersdata/ihs5_consumption_aggregate.dta")
ihs5 <- ihs5 |>
  left_join(ihs5coords, by = "case_id")
# collapse to EA_CODE
ihs5ea <- ihs5 |>
  group_by(EA_CODE) |>
  # Note that this is a weighted mean!
  summarize(poor = stats::weighted.mean(x = poor, w = hh_wgt*adulteq, na.rm = TRUE), # weighted mean
    total_weights = sum(hh_wgt*adulteq, na.rm = TRUE), # sum total weights
    total_obs = n()) # total observations (households) in the EA
head(ihs5ea)
# save it!
write_csv(ihs5ea, "rastersdata/ihs5ea.csv")

```











# One last set of data: MOSAIKS

## {background-image="rastersassets/mosaikbackground.png"}





## One last set of data: MOSAIKS

- MOSAIKS is a dataset that has a lot of different variables
  - These variables were constructed by the authors from satellite imagery
  - You can download all of these features using their website
  - [https://api.mosaiks.org](https://api.mosaiks.org) - you'll have to register
  - The data is quite large, so I've downloaded and uploaded it for you
    - It's a random selection of 500 variables for EAs in Norther Malawi
    - `rastersdata/mosaikvars.csv`
    - Note that I used `st_join` to match it to the shapefile!

[Rolf et al. (2021)](https://www.nature.com/articles/s41467-021-24638-z)








# Putting it all together



## What have we downloaded?

- We have:
  - Population data from WorldPop
  - Nightlights data from GEE
  - NDVI data from GEE
  - MOSAIKS data
  - Survey data from Malawi<br><br>
  
- We have everything we need to estimate a simple SAE model using geospatial data!
  - Note: in practice, I would use even more predictors, but this is a good start



## First, let's load all the data

```{r}
#| eval: true
#| echo: true
#| code-fold: show

# Population 
pop <- read_csv("rastersdata/mwpopEAs.csv")
# Nightlights
ntl <- read_csv("rastersdata/mwntlEAs.csv")
# NDVI
ndvi <- read_csv("rastersdata/ndviclean.csv")
# mosaiks
mosaik <- read_csv("rastersdata/mosaikvars.csv")
# survey data
ihs5ea <- read_csv("rastersdata/ihs5ea.csv")
# all EAs
adm4 <- read_sf("rastersdata/mw4.shp")
# no geometry, just EA_CODE
adm4 <- as_tibble(adm4) |>
  dplyr::select(EA_CODE, TA_CODE)

```





## Now, we join it all!

```{r}
#| eval: false
#| echo: true
#| code-fold: show

adm4 <- adm4 |>
  left_join(ihs5ea, by = "EA_CODE") |> # add survey data
  left_join(pop, by = "EA_CODE") |> # add pop
  left_join(ntl, by = "EA_CODE") |> # add nightlights
  left_join(ndvi, by = "EA_CODE") |> # add ndvi
  left_join(mosaik, by = "EA_CODE") |> # add mosaik
  
head(adm4)

```
- Oh no! This throws an error!
  - `x$EA_CODE` is a `<character>`
  - `y$EA_CODE` is a `<double>`
  - What's going on?





## Now, we join it all!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

adm4 <- adm4 |>
  mutate(EA_CODE = as.numeric(EA_CODE)) |>
  left_join(ihs5ea |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add survey data
  left_join(pop |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add pop
  left_join(ntl |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add nightlights
  left_join(ndvi |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") |> # add ndvi
  left_join(mosaik |> mutate(EA_CODE = as.numeric(EA_CODE)), by = "EA_CODE") # add mosaik
  
head(adm4)

```








## What do poverty rates look like?

```{r}
#| eval: true
#| echo: true
#| code-fold: show

# levels vs. arcsin squareroot transformation
ggplot() +
  geom_density(data = adm4, aes(x = poor), fill = "navy", alpha = 0.5) +
  geom_density(data = adm4, aes(x = asin(sqrt(poor))), fill = "orange", alpha = 0.5) +
  theme_bw()
# Not perfect but neither is horribly non-normal!

```










## Before SAE, need to do some cleaning

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# check for missings
sum(is.na(adm4$pop))
sum(is.na(adm4$ntl))
sum(is.na(adm4$ndvimin))
sum(is.na(adm4$mosaik1))
# we have missings for the mosaik features!


```









## Missing mosaiks, what to do?

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# I am going to replace missing mosaiks values with the TA mean
adm4 <- adm4 |>
  group_by(TA_CODE) |>
  mutate(across(starts_with("mosaik"), ~replace_na(., mean(., na.rm = TRUE)))) |>
  ungroup()

sum(is.na(adm4$mosaik1)) # fixed!
```









## Now let's select features

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll
library(glmnet)

# let's also log population!
adm4 <- adm4 |>
  mutate(pop = log(pop))

surveyeas <- adm4 |>
  filter(!is.na(poor))
samplefeatures <- surveyeas |>
  select(-c(EA_CODE, TA_CODE, poor, total_weights, total_obs)) # remove these variables
samplelabels <- surveyeas$poor
sampleweights <- surveyeas$total_weights

# now lasso
set.seed(24826)
lasso <- cv.glmnet(x = as.matrix(samplefeatures), y = samplelabels, 
  weights = sampleweights,
  alpha = 1)

```









## What did lasso do?

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

coef(lasso, s = "lambda.min")

```









## We want to get the non-zero variables

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

coefs <- coef(lasso, s = "lambda.min")
nonzerocoefs <- row.names(coefs)[which(coefs!=0)]
nonzerocoefs
# but we don't want the intercept here (you'll see why)
nonzerocoefs <- nonzerocoefs[-1]
nonzerocoefs
```








## Now we want to turn that into a formula!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

formula <- as.formula(paste("poor ~", paste(nonzerocoefs, collapse = " + ")))
formula
# you can see it works with a simple linear regression!
lm(formula, data = surveyeas)
```









## Finally, we can do SAE

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll
library(povmap)

saeresults <- povmap::ebp(fixed = formula, # the formula
  pop_data = adm4,
  pop_domains = "TA_CODE",
  smp_data = surveyeas,
  smp_domains = "TA_CODE",
  transformation = "arcsin",
  weights = "total_weights",
  pop_weights = "pop",
  weights_type = "nlme",
  MSE = TRUE,
  rescale_weights = TRUE,
  seed = 1234,
  L = 0)

```








## The results

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# What is R2?
summary(saeresults)$coeff_determ

```

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# Relatively normal?
summary(saeresults)$normality

```







## Now let's get the predictions

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# get the predictions
hat <- as_tibble(saeresults$ind)
hat <- hat |>
  select(TA_CODE = Domain, poor = Mean)
# We also want variance estimates
mse <- as_tibble(saeresults$MSE) |>
  select(TA_CODE = Domain, mse = Mean)
# note to get the standard error we have to take the square root!
mse <- mse |>
  mutate(se = sqrt(mse))

# finally, join them!
final <- left_join(hat, mse, by = "TA_CODE")

head(final)
```







## Let's map it!

```{r}
#| eval: true
#| echo: true
#| code-fold: show
#| class-output: hscroll

# Admin3 (TA)
adm3 <- read_sf("rastersdata/mw3.shp")
adm3 <- adm3 |>
  left_join(final, by = "TA_CODE")

# let's output two things: poverty rate and CV (se/mean)
g1 <- ggplot() +
  geom_sf(data = adm3, aes(fill = poor), color = "white") +
  scale_fill_distiller("Poverty Rate", palette = "Spectral", direction = -1) +
  theme_bw() +
  theme(legend.position = "bottom")
g2 <- ggplot() +
  geom_sf(data = adm3, aes(fill = se/poor), color = "white") +
  scale_fill_distiller("CV", palette = "Spectral", direction = -1) +
  theme_bw() +
  theme(legend.position = "bottom")
g3 <- ggplot() +
  geom_sf(data = adm3, aes(fill = as.factor((se/poor)<=0.4)), color = "white") +
  scale_fill_brewer("CV below 0.4?", palette = "Reds", direction = -1) +
  theme_bw() +
  theme(legend.position = "bottom")

```






## The final result!

```{r}
#| eval: true
#| echo: true
#| code-fold: true
#| fig-width: 17
library(cowplot)

plot_grid(g1, g2, g3, ncol = 3)
```






## And here it is with Rumphi

```{r}
#| eval: true
#| echo: true
#| code-fold: true
#| fig-width: 17

rumphibbox <- st_bbox(adm3 |> filter(DIST_CODE=="107"))
g2new <- ggdraw() +
  draw_plot(
    {
      g1 +
        coord_sf(
          xlim = c(rumphibbox[1], rumphibbox[3]),
          ylim = c(rumphibbox[2], rumphibbox[4]),
          expand = FALSE) +
        labs(subtitle = "Rumphi only")
    }
  )
g1new <- g1 + 
  labs(subtitle = "Northern Malawi") +
  geom_sf(data = st_as_sf(st_as_sfc(rumphibbox)), fill = NA, color = "black")
plot_grid(g1new, g2new, ncol = 2)

```



















